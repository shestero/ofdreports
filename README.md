## Краткое описание 
##### (относится к части №1 задания)

Функционал выбора и запуска отчётов реализует объект `ReportManager` (объект управляющий отчётами в приложении).
* Отчёты там регистрируются в коллекции `reports`. 
* Метод `runReport` запускает отчёт по имени с предварительной проверкой (см. ниже - *check*).
* Метод `ShowReportList`  - выводит список зарегистрированных отчётов.

Отчёты делаются из трейта `Report0`.
В нём надо перекрыть следующие методы:
* `shortName` - короткое кодовое имя отчёта, используемое менеджером (ReportManager см. ниже);
* `shortDescr` - краткое описание;
* `longDescr` - подробное описание (по умолчанию = краткому);
* `check` - опцинальная процедура, выполняющая проверку возможности запуска отчёта (возвращает true если запуск разрешён);
* `run` - соственно выполняет отчёт.

Отчёты могут принимать параметры, их надо перечислить коллекции RequiredParams.
`paramsDescr` - небольшая вспомогательная процедура, автоматически генерирующая список параметров отчёта для пользователя.


## Технические замечения
##### (относится в основном к части №2 задания) - пример отчёта farma.
Для разработки я использовал IntelliJ IDEA Community Edition последней версии 2019.2.2, SBT 1.3.0. Для сборки Jar использовал plugin SBT sbt-assembly версии 0.14.10.

Для инициализации проверочных данных я сделал вспомогательную программу odfinit1, которая записывает из локальных CSV файлов "inn_info.farma", "misc.adm_area" в Hive/parquet, а "receipts" тиражирует в HDFS в /user/big/receipt/ с 1 января по 10 апреля 2018, а также созаёт соответствующие базы в Hive.
Позже я переделал её в *фиктивный* отчёт «farmainit».

Аналитический помесячный отчёт по рынке фармацептики принимает параметр month=YYYY-MM, на выходе получается таблица Hive reports.farma_YYYY_MM.


Для хранилища Hive я использовал Docker-контейнер `sequenceiq/hadoop-docker`, подключённый к 9900-му порту (на 9000 у меня Clickhouse).

Для спецификации HDFS, если она не настроена как доступная по «/» используется параметр отчётов «hive».
Например, инициализация исходных данных на локальном «spark-warehouse» может быть произведена так:
`spark-submit /home/shestero/ofdreports/target/scala-2.12/ofdreports-assembly-0.1.jar farmainit hdfs=hdfs://localhost:9900/`
(предполагается, что парамер «hive» может быть использован многими из отчётов, поэтому он имеет значение по-умолчанию).

Отчёт может быть запущен, например так:
`spark-submit --conf hive.metastore.uris=thrift://172.17.0.3:9083 /home/shestero/ofdreports/target/scala-2.12/ofdreports-assembly-0.1.jar farma month=2018-01 hdfs=hdfs://localhost:9900/`

Работать с удалённым Hive (использовался Docker-контейнет `cloudera/quickstart`) я тоже пробовал, прописав .set("hive.metastore.uris", "thrift://quickstart.cloudera:9083") в коде.

PS Для проверки был использован дистрибутив spark-2.4.2-bin-hadoop2.7.tgz , так как только версия 2.4.2 поставляется предкомпилированной для Scala 2.12, которую я использовал.
